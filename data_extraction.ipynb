{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42223754",
   "metadata": {},
   "source": [
    "1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8830b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import VARCHAR, DOUBLE, INTEGER, DATE, TIME\n",
    "from sqlalchemy.dialects.mysql import TINYINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae8253",
   "metadata": {},
   "source": [
    "2. Data Import from CSV and XLSX Files and Export to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa6cb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2024.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2023.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2022.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2020.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2021.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2019.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2018.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2017.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2016.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2015.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2014.xlsx',\n",
       " '../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data/ttc-streetcar-delay-data-2025-01-05.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the Delay Data Excel file\n",
    "files_path = \"../../../../../../Volumes/LACIE SHARE/ML_Data_Projects/Toronto_Transit_Dashboards/TTC_Transit_Data/Streetcar_Delay_Data\"\n",
    "# Use the path to comiple list of all pertinent files (excel and csv)\n",
    "excel_files = glob.glob(os.path.join(files_path, \"*.xlsx\"))\n",
    "csv_files = glob.glob(os.path.join(files_path, \"*.csv\"))\n",
    "all_files = excel_files + csv_files\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6efac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the database connection\n",
    "engine = create_engine('mysql+pymysql://root:archit14411@localhost:3306/ttc_delay_data_db?charset=utf8mb4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "192d5f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 PROCESSING 11 EXCEL FILES ONLY\n",
      "Processing ttc-streetcar-delay-data-2024.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2024.xlsx: 14206\n",
      "Cleaned data shape: (14206, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2024.xlsx processed - 13992 rows inserted, 214 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2023.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2023.xlsx: 14413\n",
      "Cleaned data shape: (14413, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2023.xlsx processed - 14240 rows inserted, 173 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2022.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2022.xlsx: 17655\n",
      "Cleaned data shape: (17655, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2022.xlsx processed - 17479 rows inserted, 176 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2020.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2020.xlsx: 7830\n",
      "Cleaned data shape: (7830, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2020.xlsx processed - 7737 rows inserted, 93 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2021.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2021.xlsx: 14596\n",
      "Cleaned data shape: (14596, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2021.xlsx processed - 14389 rows inserted, 207 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2019.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2019.xlsx: 11882\n",
      "Cleaned data shape: (11882, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2019.xlsx processed - 11758 rows inserted, 124 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2018.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2018.xlsx: 15612\n",
      "Cleaned data shape: (15612, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2018.xlsx processed - 15360 rows inserted, 252 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2017.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2017.xlsx: 13762\n",
      "Cleaned data shape: (13762, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2017.xlsx processed - 13626 rows inserted, 136 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2016.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2016.xlsx: 14021\n",
      "Cleaned data shape: (14021, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2016.xlsx processed - 13864 rows inserted, 157 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2015.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2015.xlsx: 12221\n",
      "Cleaned data shape: (12221, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2015.xlsx processed - 12066 rows inserted, 155 rows skipped\n",
      "Processing ttc-streetcar-delay-data-2014.xlsx...\n",
      "Total rows loaded from Excel ttc-streetcar-delay-data-2014.xlsx: 11027\n",
      "Cleaned data shape: (11027, 10)\n",
      "Columns kept: ['report_date', 'route', 'time', 'day', 'location', 'incident', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "✅ ttc-streetcar-delay-data-2014.xlsx processed - 10861 rows inserted, 166 rows skipped\n",
      "\n",
      "🎉 Excel files processed!\n",
      "Total rows imported: 145372\n",
      "Total rows skipped: 1853\n"
     ]
    }
   ],
   "source": [
    "# We start by working with the excel files only (2014-2024 data).\n",
    "# Create counters for imported and skipped rows\n",
    "total_imported = 0\n",
    "total_skipped = 0\n",
    "\n",
    "print(f\"🎯 PROCESSING {len(excel_files)} EXCEL FILES ONLY\")\n",
    "\n",
    "# Iterate through each Excel file\n",
    "for file in excel_files:\n",
    "    print(f\"Processing {os.path.basename(file)}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load Excel file and process each sheet individually\n",
    "        xls = pd.ExcelFile(file)\n",
    "        \n",
    "        # Process each sheet individually to handle column name variations\n",
    "        sheet_dataframes = []\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            sheet_df = xls.parse(sheet_name)\n",
    "            \n",
    "            # Standardize column names within each sheet before concatenating\n",
    "            column_mapping = {\n",
    "                # Date variations\n",
    "                'Date': 'Report Date',\n",
    "                # Route variations  \n",
    "                'Line': 'Route',\n",
    "                # Direction variations\n",
    "                'Bound': 'Direction',\n",
    "                # Delay variations\n",
    "                'Delay': 'Min Delay',\n",
    "                # Gap variations\n",
    "                'Gap': 'Min Gap'\n",
    "            }\n",
    "            \n",
    "            # Apply column mapping\n",
    "            sheet_df = sheet_df.rename(columns=column_mapping)\n",
    "            sheet_dataframes.append(sheet_df)\n",
    "        \n",
    "        # Now concatenate the standardized sheets\n",
    "        delay_data = pd.concat(sheet_dataframes, ignore_index=True)\n",
    "        print(f\"Total rows loaded from Excel {os.path.basename(file)}: {delay_data.shape[0]}\")\n",
    "        \n",
    "        # Changing the date columns to be more sutiable for SQL.\n",
    "        if \"Report Date\" in delay_data.columns:\n",
    "            delay_data[\"Report Date\"] = delay_data[\"Report Date\"].dt.date\n",
    "        \n",
    "        # Clean column names to match database schema\n",
    "        delay_data.columns = delay_data.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "        \n",
    "        # Define the expected database columns, to weed out any unexpected columns.\n",
    "        expected_columns = [\n",
    "            'report_date', 'route', 'time', 'day', 'location', \n",
    "            'incident', 'min_delay', 'min_gap', 'direction', 'vehicle'\n",
    "        ]\n",
    "        \n",
    "        # Keep only expected columns\n",
    "        available_columns = [col for col in expected_columns if col in delay_data.columns]\n",
    "        delay_data = delay_data[available_columns]\n",
    "        \n",
    "        print(f\"Cleaned data shape: {delay_data.shape}\")\n",
    "        print(f\"Columns kept: {available_columns}\")\n",
    "        \n",
    "        # Insert row-by-row to handle constraint violations\n",
    "        successful_inserts = 0\n",
    "        \n",
    "        for idx, row in delay_data.iterrows():\n",
    "            try:\n",
    "                row.to_frame().T.to_sql(\n",
    "                    name='streetcar_delay_data',\n",
    "                    con=engine,\n",
    "                    if_exists='append',\n",
    "                    index=False\n",
    "                )\n",
    "                successful_inserts += 1\n",
    "            except:\n",
    "                pass  # Silently skip bad rows\n",
    "        \n",
    "        file_skipped = len(delay_data) - successful_inserts\n",
    "        total_imported += successful_inserts\n",
    "        total_skipped += file_skipped\n",
    "        \n",
    "        print(f\"✅ {os.path.basename(file)} processed - {successful_inserts} rows inserted, {file_skipped} rows skipped\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {os.path.basename(file)}: {e}\")\n",
    "        continue\n",
    "\n",
    "engine.dispose()\n",
    "print(f\"\\n🎉 Excel files processed!\")\n",
    "print(f\"Total rows imported: {total_imported}\")\n",
    "print(f\"Total rows skipped: {total_skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8c1717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 PROCESSING 1 CSV FILES ONLY\n",
      "Processing ttc-streetcar-delay-data-2025-01-05.csv...\n",
      "Total rows loaded from CSV ttc-streetcar-delay-data-2025-01-05.csv: 6462\n",
      "Original columns: ['_id', 'Date', 'Line', 'Time', 'Day', 'Station', 'Code', 'Min Delay', 'Min Gap', 'Bound', 'Vehicle']\n",
      "🗑️  Dropped unwanted columns: ['_id', 'Code']\n",
      "Converting Date column from: object\n",
      "✅ Date converted and renamed to 'Report Date'\n",
      "Line values before trimming: ['504 KING', '506 CARLTON', '504 KING']\n",
      "✅ Line trimmed to 3 digits and renamed to 'Route'\n",
      "Route values after trimming: ['504', '506', '504']\n",
      "✅ Renamed 'Station' to 'Location'\n",
      "✅ Renamed 'Bound' to 'Direction'\n",
      "Columns after standardization: ['Report Date', 'Route', 'Time', 'Day', 'Location', 'Min Delay', 'Min Gap', 'Direction', 'Vehicle']\n",
      "Database-ready columns: ['report_date', 'route', 'time', 'day', 'location', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "Available columns: ['report_date', 'route', 'time', 'day', 'location', 'min_delay', 'min_gap', 'direction', 'vehicle']\n",
      "Missing columns: ['incident']\n",
      "Extra columns (will be dropped): []\n",
      "Final data shape: (6462, 9)\n",
      "✅ ttc-streetcar-delay-data-2025-01-05.csv processed - 6448 rows inserted, 14 rows skipped\n",
      "\n",
      "🎉 CSV files processed!\n",
      "Total rows imported: 6448\n",
      "Total rows skipped: 14\n"
     ]
    }
   ],
   "source": [
    "# Now we will process the CSV file (2025 data).\n",
    "# Reset the counters for imported and skipped rows.\n",
    "total_imported = 0\n",
    "total_skipped = 0\n",
    "\n",
    "print(f\"🎯 PROCESSING {len(csv_files)} CSV FILES ONLY\")\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f\"Processing {os.path.basename(file)}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load CSV file\n",
    "        delay_data = pd.read_csv(file)\n",
    "        print(f\"Total rows loaded from CSV {os.path.basename(file)}: {delay_data.shape[0]}\")\n",
    "        print(f\"Original columns: {list(delay_data.columns)}\")\n",
    "        \n",
    "        # CSV-specific preprocessing: Remove unwanted columns FIRST\n",
    "        unwanted_columns = ['_id', 'id', 'Code', 'code']\n",
    "        columns_to_drop = [col for col in unwanted_columns if col in delay_data.columns]\n",
    "        if columns_to_drop:\n",
    "            delay_data = delay_data.drop(columns=columns_to_drop)\n",
    "            print(f\"🗑️  Dropped unwanted columns: {columns_to_drop}\")\n",
    "        \n",
    "        # Handle CSV date column (string format)\n",
    "        if \"Date\" in delay_data.columns:\n",
    "            print(f\"Converting Date column from: {delay_data['Date'].dtype}\")\n",
    "            delay_data[\"Date\"] = pd.to_datetime(delay_data[\"Date\"]).dt.date\n",
    "            delay_data = delay_data.rename(columns={\"Date\": \"Report Date\"})\n",
    "            print(f\"✅ Date converted and renamed to 'Report Date'\")\n",
    "        \n",
    "        # CSV-specific Line/Route handling - trim to first 3 digits\n",
    "        if \"Line\" in delay_data.columns:\n",
    "            print(f\"Line values before trimming: {delay_data['Line'].head(3).tolist()}\")\n",
    "            delay_data[\"Line\"] = delay_data[\"Line\"].astype(str).str[:3]\n",
    "            delay_data = delay_data.rename(columns={\"Line\": \"Route\"})\n",
    "            print(f\"✅ Line trimmed to 3 digits and renamed to 'Route'\")\n",
    "            print(f\"Route values after trimming: {delay_data['Route'].head(3).tolist()}\")\n",
    "        \n",
    "        # Handle other column standardizations\n",
    "        column_mapping = {\n",
    "            'Station': 'Location',\n",
    "            'Bound': 'Direction',\n",
    "            'Delay': 'Min Delay', \n",
    "            'Gap': 'Min Gap'\n",
    "        }\n",
    "        \n",
    "        for old_col, new_col in column_mapping.items():\n",
    "            if old_col in delay_data.columns:\n",
    "                delay_data = delay_data.rename(columns={old_col: new_col})\n",
    "                print(f\"✅ Renamed '{old_col}' to '{new_col}'\")\n",
    "        \n",
    "        print(f\"Columns after standardization: {list(delay_data.columns)}\")\n",
    "        \n",
    "        # Clean column names to match database schema\n",
    "        delay_data.columns = delay_data.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "        print(f\"Database-ready columns: {list(delay_data.columns)}\")\n",
    "        \n",
    "        # Define the expected database columns\n",
    "        expected_columns = [\n",
    "            'report_date', 'route', 'time', 'day', 'location', \n",
    "            'incident', 'min_delay', 'min_gap', 'direction', 'vehicle'\n",
    "        ]\n",
    "        \n",
    "        # Keep only expected columns\n",
    "        available_columns = [col for col in expected_columns if col in delay_data.columns]\n",
    "        missing_columns = [col for col in expected_columns if col not in delay_data.columns]\n",
    "        extra_columns = [col for col in delay_data.columns if col not in expected_columns]\n",
    "        \n",
    "        print(f\"Available columns: {available_columns}\")\n",
    "        print(f\"Missing columns: {missing_columns}\")\n",
    "        print(f\"Extra columns (will be dropped): {extra_columns}\")\n",
    "        \n",
    "        delay_data = delay_data[available_columns]\n",
    "        \n",
    "        print(f\"Final data shape: {delay_data.shape}\")\n",
    "        \n",
    "        # Insert row-by-row to handle constraint violations\n",
    "        successful_inserts = 0\n",
    "        \n",
    "        for idx, row in delay_data.iterrows():\n",
    "            try:\n",
    "                row.to_frame().T.to_sql(\n",
    "                    name='streetcar_delay_data',\n",
    "                    con=engine,\n",
    "                    if_exists='append',\n",
    "                    index=False\n",
    "                )\n",
    "                successful_inserts += 1\n",
    "            except Exception as e:\n",
    "                # For debugging, you can uncomment this line:\n",
    "                # print(f\"Row {idx} failed: {e}\")\n",
    "                pass\n",
    "        \n",
    "        file_skipped = len(delay_data) - successful_inserts\n",
    "        total_imported += successful_inserts\n",
    "        total_skipped += file_skipped\n",
    "        \n",
    "        print(f\"✅ {os.path.basename(file)} processed - {successful_inserts} rows inserted, {file_skipped} rows skipped\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {os.path.basename(file)}: {e}\")\n",
    "        continue\n",
    "\n",
    "engine.dispose()\n",
    "print(f\"\\n🎉 CSV files processed!\")\n",
    "print(f\"Total rows imported: {total_imported}\")\n",
    "print(f\"Total rows skipped: {total_skipped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34725a",
   "metadata": {},
   "source": [
    "3. Data Import from Database into Excel File (For use with Tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a01d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
